{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9O8VOTiShbZ"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AppimateSA/AutoVisual/blob/text2video_stablediffusion/notebooks/Text-To-Video-Finetuning.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FPL6HGLShbb"
      },
      "source": [
        "# Prepare the enviornment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sl9M30V4Shbb",
        "outputId": "7082a1d8-031f-4e39-bd79-b66ffae3b49a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import platform\n",
        "import requests\n",
        "import torch\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    WORKING_DIR = '.'\n",
        "    IN_COLAB = False\n",
        "if IN_COLAB:\n",
        "    WORKING_DIR = '/content'\n",
        "    drive.mount('/content/drive',  force_remount=True) # Mount drive in order access Google drive\n",
        "if IN_COLAB:\n",
        "    sys.path.insert(0, WORKING_DIR)\n",
        "else:\n",
        "    # The actual code is one level higher in folder depth/structure, so we're elevating this notebook.\n",
        "    sys.path.insert(0,f\"{WORKING_DIR}/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1412IyZUKUJ"
      },
      "source": [
        "# Install"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GdqePw3Shbc"
      },
      "source": [
        "### Install Main Repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # %cd {WORKING_DIR}/drive/MyDrive/Colab Notebooks/Supervised Learning/T2V/Text-To-Video-Finetuning/\n",
        "# %cd WORKING_DIR\n",
        "\n",
        "dataFolder = os.path.expanduser('~/Desktop/AutoVisual')\n",
        "# %cd {dataFolder}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkZn2d3-UbFM"
      },
      "source": [
        "### Install Main Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5cN5AOw_Yf3",
        "outputId": "debd8a8f-6cc9-481b-f642-a2ae63b8dedb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated Git hooks.\n",
            "Git LFS initialized.\n",
            "Cloning into './models/model_scope_diffusers'...\n",
            "remote: Enumerating objects: 79, done.\u001b[K\n",
            "remote: Total 79 (delta 0), reused 0 (delta 0), pack-reused 79\u001b[K\n",
            "Unpacking objects: 100% (79/79), 523.71 KiB | 2.34 MiB/s, done.\n",
            "Filtering content: 100% (12/12), 12.51 GiB | 2.37 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!git lfs install\n",
        "!git clone https://huggingface.co/damo-vilab/text-to-video-ms-1.7b ./models/model_scope_diffusers/\n",
        "# # %cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awu5Sjm7LFqc"
      },
      "source": [
        "## Preprocess Videos\n",
        "See arguments you can pass [here](https://github.com/ExponentialML/Video-BLIP2-Preprocessor#default-arguments)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFdu2IK6BZcj",
        "outputId": "11ece480-8cab-43c9-e691-c9e052ceeb59"
      },
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "    dataFolder = f'{WORKING_DIR}/drive/MyDrive/Colab Notebooks/datasets/Appimate'\n",
        "else:\n",
        "    dataFolder = os.path.expanduser( '~/.cache/datasets/Appimate' )\n",
        "    \n",
        "# print(dataFolder)\n",
        "# !pwd\n",
        "# # %cd {WORKING_DIR}/AutoVisual\n",
        "# !python preprocess.py \\\n",
        "#     --input_dir '{dataFolder}' \\\n",
        "#     --json_file_path '{dataFolder}/dataset.json' \\\n",
        "#     --skip_interval 1 \\\n",
        "#     --output_dir '{dataFolder}/T2V'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0I4dk_Mnajp4"
      },
      "source": [
        "## Train Video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKG_b7qua7SX",
        "outputId": "7ab65872-74b7-4a37-8b35-8817ba3802be"
      },
      "outputs": [],
      "source": [
        "# %%writefile ./configs/my_config_hq.yaml\n",
        "\n",
        "# pretrained_model_path: \"/content/text-to-video-ms-1.7b\" #https://huggingface.co/damo-vilab/text-to-video-ms-1.7b/tree/main\n",
        "# output_dir: \"/content/drive/MyDrive/Colab Notebooks/Supervised Learning/T2V/AutoVisual/outputs\"\n",
        "# train_text_encoder: False\n",
        "# #resume_from_checkpoint: None\n",
        "# dataset_types:\n",
        "#   - 'folder'\n",
        "\n",
        "# train_data:\n",
        "#   path: \"/content/train_all_mp4\"\n",
        "#   preprocessed: True\n",
        "#   n_sample_frames: 2\n",
        "#   shuffle_frames: False\n",
        "#   width: 128 #384\n",
        "#   height: 128 #256\n",
        "#   sample_start_idx: 0\n",
        "#   sample_frame_rate: 24\n",
        "#   vid_data_key: \"video_path\"\n",
        "\n",
        "#   # single_video_path: \"\"\n",
        "#   single_video_prompt: \"\"\n",
        "\n",
        "# validation_data:\n",
        "#   prompt: \"\"\n",
        "#   sample_preview: True\n",
        "#   num_frames: 16\n",
        "#   width: 128 #384\n",
        "#   height: 128 #256\n",
        "#   num_inference_steps: 50\n",
        "#   guidance_scale: 9\n",
        "\n",
        "# learning_rate: 1e-5\n",
        "# adam_weight_decay: 1e-2\n",
        "# train_batch_size: 1\n",
        "# max_train_steps: 50000\n",
        "# checkpointing_steps: 2500\n",
        "# validation_steps: 2500\n",
        "# trainable_modules:\n",
        "#   - \"attentions\"\n",
        "# seed: 64\n",
        "# mixed_precision: \"fp16\"\n",
        "# use_8bit_adam: False # This seems to be incompatible at the moment.\n",
        "\n",
        "# gradient_checkpointing: True\n",
        "# text_encoder_gradient_checkpointing: True\n",
        "# # Xformers must be installed\n",
        "# enable_xformers_memory_efficient_attention: False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iX1eFvYTLldG",
        "outputId": "c827cf29-a327-410d-cc66-0741fbc7047c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/luthando/miniconda3/envs/vgen/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'libc10_cuda.so: cannot open shared object file: No such file or directory'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
            "  warn(\n",
            "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
            "    PyTorch 2.2.0+cu121 with CUDA 1201 (you have 2.2.0)\n",
            "    Python  3.8.18 (you have 3.8.18)\n",
            "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
            "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
            "  Set XFORMERS_MORE_DETAILS=1 for more details\n",
            "Initializing the conversion map\n",
            "03/27/2024 15:37:30 - INFO - __main__ - Distributed environment: NO\n",
            "Num processes: 1\n",
            "Process index: 0\n",
            "Local process index: 0\n",
            "Device: cpu\n",
            "\n",
            "Mixed precision type: fp16\n",
            "\n",
            "/home/luthando/miniconda3/envs/vgen/lib/python3.8/site-packages/diffusers/configuration_utils.py:217: FutureWarning: It is deprecated to pass a pretrained model name or path to `from_config`.If you were trying to load a scheduler, please use <class 'diffusers.schedulers.scheduling_ddpm.DDPMScheduler'>.from_pretrained(...) instead. Otherwise, please make sure to pass a configuration dictionary instead. This functionality will be removed in v1.0.0.\n",
            "  deprecate(\"config-passed-as-path\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
            "{'variance_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "{'downsample_padding', 'mid_block_scale_factor'} was not found in config. Values will be initialized to default values.\n",
            "train_data {'path': '/home/luthando/.cache/datasets/Appimate/T2V', 'preprocessed': True, 'n_sample_frames': 2, 'shuffle_frames': False, 'width': 384, 'height': 256, 'sample_start_idx': 0, 'sample_frame_rate': 24, 'vid_data_key': 'video_path', 'single_video_prompt': ''}\n",
            "dataset_types ['folder']\n",
            "json\n",
            "dataset_types ['folder']\n",
            "single_video\n",
            "dataset_types ['folder']\n",
            "image\n",
            "dataset_types ['folder']\n",
            "folder\n",
            "dataset type folder\n",
            "--------------------- 1\n",
            "text_enable True\n",
            "03/27/2024 15:37:31 - INFO - __main__ - ***** Running training *****\n",
            "03/27/2024 15:37:31 - INFO - __main__ -   Num examples = 10\n",
            "03/27/2024 15:37:31 - INFO - __main__ -   Num Epochs = 25\n",
            "03/27/2024 15:37:31 - INFO - __main__ -   Instantaneous batch size per device = 1\n",
            "03/27/2024 15:37:31 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "03/27/2024 15:37:31 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
            "03/27/2024 15:37:31 - INFO - __main__ -   Total optimization steps = 250\n",
            "Steps:   0%|                                            | 0/250 [00:00<?, ?it/s]832 params have been processed.\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 1032, in <module>\n",
            "    main(**config)\n",
            "  File \"train.py\", line 874, in main\n",
            "    loss, latents = finetune_unet(batch, train_encoder=train_text_encoder)\n",
            "  File \"train.py\", line 759, in finetune_unet\n",
            "    latents = tensor_to_vae_latent(pixel_values, vae)\n",
            "  File \"train.py\", line 350, in tensor_to_vae_latent\n",
            "    latents = vae.encode(t).latent_dist.sample()\n",
            "  File \"/home/luthando/miniconda3/envs/vgen/lib/python3.8/site-packages/diffusers/utils/accelerate_utils.py\", line 46, in wrapper\n",
            "    return method(self, *args, **kwargs)\n",
            "  File \"/home/luthando/miniconda3/envs/vgen/lib/python3.8/site-packages/diffusers/models/autoencoder_kl.py\", line 233, in encode\n",
            "    encoded_slices = [self.encoder(x_slice) for x_slice in x.split(1)]\n",
            "  File \"/home/luthando/miniconda3/envs/vgen/lib/python3.8/site-packages/diffusers/models/autoencoder_kl.py\", line 233, in <listcomp>\n",
            "    encoded_slices = [self.encoder(x_slice) for x_slice in x.split(1)]\n",
            "  File \"/home/luthando/miniconda3/envs/vgen/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/home/luthando/miniconda3/envs/vgen/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/home/luthando/miniconda3/envs/vgen/lib/python3.8/site-packages/diffusers/models/vae.py\", line 110, in forward\n",
            "    sample = self.conv_in(sample)\n",
            "  File \"/home/luthando/miniconda3/envs/vgen/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/home/luthando/miniconda3/envs/vgen/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/home/luthando/miniconda3/envs/vgen/lib/python3.8/site-packages/torch/nn/modules/conv.py\", line 460, in forward\n",
            "    return self._conv_forward(input, self.weight, self.bias)\n",
            "  File \"/home/luthando/miniconda3/envs/vgen/lib/python3.8/site-packages/torch/nn/modules/conv.py\", line 456, in _conv_forward\n",
            "    return F.conv2d(input, weight, bias, self.stride,\n",
            "RuntimeError: Input type (float) and bias type (c10::Half) should be the same\n",
            "Steps:   0%|                                            | 0/250 [00:00<?, ?it/s]\n"
          ]
        }
      ],
      "source": [
        "!python train.py --config ./configs/my_config_hq.yaml --experiment_num=0 --use_wandb=False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DotzCfQbShbf"
      },
      "source": [
        "# Test Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6iy3bIOhfgE",
        "outputId": "4296fae8-b85a-49e9-8198-27c29cf81f79"
      },
      "outputs": [],
      "source": [
        "# !python inference.py \\\n",
        "#     --model '/content/drive/MyDrive/Colab Notebooks/Supervised Learning/T2V/Text-To-Video-Finetuning/outputs/train_2024-01-13T20-29-09/checkpoint-50\" --prompt \"Simplify (x+1)(x-1) explained' \\\n",
        "#     --prompt 'Please simplify this expression: (x+1)(x-1)' \\\n",
        "#     --neg_prompt \"watermark+++, text, shutterstock text, shutterstock++, blurry, ugly, username, url, low resolution, low quality\" \\\n",
        "#     --num-frames 16 \\\n",
        "#     --window-size 12 \\\n",
        "#     --width 128 \\\n",
        "#     --height 128 \\\n",
        "#     --sdp"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuClass": "premium",
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
