{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9O8VOTiShbZ"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AppimateSA/AutoVisual/blob/text2video_stablediffusion/notebooks/Text-To-Video-Finetuning.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fri Feb 23 08:55:26 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 545.29.06              Driver Version: 545.29.06    CUDA Version: 12.3     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA Graphics Device         Off | 00000000:01:00.0  On |                  N/A |\n",
            "|  0%   33C    P5              26W / 285W |  11870MiB / 16376MiB |     21%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|    0   N/A  N/A      2137      G   /usr/lib/xorg/Xorg                          235MiB |\n",
            "|    0   N/A  N/A      2272      G   /usr/bin/gnome-shell                         88MiB |\n",
            "|    0   N/A  N/A      3475      G   ...sion,SpareRendererForSitePerProcess       96MiB |\n",
            "|    0   N/A  N/A     10391      G   ...irefox/3836/usr/lib/firefox/firefox      327MiB |\n",
            "|    0   N/A  N/A     43765      C   ...iconda3/envs/pytorch-gpu/bin/python    11100MiB |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FPL6HGLShbb"
      },
      "source": [
        "# Prepare the enviornment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sl9M30V4Shbb",
        "outputId": "7082a1d8-031f-4e39-bd79-b66ffae3b49a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import platform\n",
        "import requests\n",
        "import torch\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    WORKING_DIR = '.'\n",
        "    IN_COLAB = False\n",
        "if IN_COLAB:\n",
        "    WORKING_DIR = '/content'\n",
        "    drive.mount('/content/drive',  force_remount=True) # Mount drive in order access Google drive\n",
        "if IN_COLAB:\n",
        "    sys.path.insert(0, WORKING_DIR)\n",
        "else:\n",
        "    # The actual code is one level higher in folder depth/structure, so we're elevating this notebook.\n",
        "    sys.path.insert(0,f\"{WORKING_DIR}/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1412IyZUKUJ"
      },
      "source": [
        "# Install"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GdqePw3Shbc"
      },
      "source": [
        "### Install Main Repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dXCcdb_-8ej-",
        "outputId": "48cf320f-1c95-46d0-9181-8bec01d057e3"
      },
      "outputs": [],
      "source": [
        "# !git clone https://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@github.com/AppimateSA/AutoVisual.git\n",
        "# %cd /content/AutoVisual\n",
        "# !git checkout text2video_stablediffusion\n",
        "# # !ls\n",
        "# !pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkZn2d3-UbFM"
      },
      "source": [
        "### Install Main Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5cN5AOw_Yf3",
        "outputId": "debd8a8f-6cc9-481b-f642-a2ae63b8dedb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/luthando/Desktop/AutoVisual\n",
            "fatal: destination path 'text-to-video-ms-1.7b' already exists and is not an empty directory.\n",
            "/home/luthando/Desktop\n"
          ]
        }
      ],
      "source": [
        "# # %cd {WORKING_DIR}/drive/MyDrive/Colab Notebooks/Supervised Learning/T2V/Text-To-Video-Finetuning/\n",
        "# %cd WORKING_DIR\n",
        "\n",
        "# dataFolder = os.path.expanduser('~/.cache/models')\n",
        "dataFolder = os.path.expanduser('~/Desktop/AutoVisual')\n",
        "%cd {dataFolder}\n",
        "# !git lfs install\n",
        "!git clone https://huggingface.co/damo-vilab/text-to-video-ms-1.7b\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awu5Sjm7LFqc"
      },
      "source": [
        "## Preprocess Videos\n",
        "See arguments you can pass [here](https://github.com/ExponentialML/Video-BLIP2-Preprocessor#default-arguments)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFdu2IK6BZcj",
        "outputId": "11ece480-8cab-43c9-e691-c9e052ceeb59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/luthando/Desktop/AutoVisual\n",
            "Data preprocessing STARTED!\n",
            "json_file_path:  /home/luthando/.cache/datasets/Appimate/dataset.json\n",
            "Data preprocessing DONE: 10 text prompts -> 10 videos\n"
          ]
        }
      ],
      "source": [
        "if IN_COLAB:\n",
        "    dataFolder = f'{WORKING_DIR}/drive/MyDrive/Colab Notebooks/datasets/Appimate'\n",
        "    # %cd {WORKING_DIR}/drive/MyDrive/Colab Notebooks/Supervised Learning/T2V/Text-To-Video-Finetuning\n",
        "else:\n",
        "    dataFolder = os.path.expanduser('~/.cache/datasets/Appimate')\n",
        "    \n",
        "%cd {WORKING_DIR}/AutoVisual\n",
        "!python preprocess.py \\\n",
        "    --input_dir '{dataFolder}' \\\n",
        "    --json_file_path '{dataFolder}/dataset.json' \\\n",
        "    --skip_interval 1 \\\n",
        "    --output_dir '{dataFolder}/T2V'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0I4dk_Mnajp4"
      },
      "source": [
        "## Train Video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKG_b7qua7SX",
        "outputId": "7ab65872-74b7-4a37-8b35-8817ba3802be"
      },
      "outputs": [],
      "source": [
        "# %%writefile ./configs/my_config_hq.yaml\n",
        "\n",
        "# pretrained_model_path: \"/content/text-to-video-ms-1.7b\" #https://huggingface.co/damo-vilab/text-to-video-ms-1.7b/tree/main\n",
        "# output_dir: \"/content/drive/MyDrive/Colab Notebooks/Supervised Learning/T2V/AutoVisual/outputs\"\n",
        "# train_text_encoder: False\n",
        "# #resume_from_checkpoint: None\n",
        "# dataset_types:\n",
        "#   - 'folder'\n",
        "\n",
        "# train_data:\n",
        "#   path: \"/content/train_all_mp4\"\n",
        "#   preprocessed: True\n",
        "#   n_sample_frames: 2\n",
        "#   shuffle_frames: False\n",
        "#   width: 128 #384\n",
        "#   height: 128 #256\n",
        "#   sample_start_idx: 0\n",
        "#   sample_frame_rate: 24\n",
        "#   vid_data_key: \"video_path\"\n",
        "\n",
        "#   # single_video_path: \"\"\n",
        "#   single_video_prompt: \"\"\n",
        "\n",
        "# validation_data:\n",
        "#   prompt: \"\"\n",
        "#   sample_preview: True\n",
        "#   num_frames: 16\n",
        "#   width: 128 #384\n",
        "#   height: 128 #256\n",
        "#   num_inference_steps: 50\n",
        "#   guidance_scale: 9\n",
        "\n",
        "# learning_rate: 1e-5\n",
        "# adam_weight_decay: 1e-2\n",
        "# train_batch_size: 1\n",
        "# max_train_steps: 50000\n",
        "# checkpointing_steps: 2500\n",
        "# validation_steps: 2500\n",
        "# trainable_modules:\n",
        "#   - \"attentions\"\n",
        "# seed: 64\n",
        "# mixed_precision: \"fp16\"\n",
        "# use_8bit_adam: False # This seems to be incompatible at the moment.\n",
        "\n",
        "# gradient_checkpointing: True\n",
        "# text_encoder_gradient_checkpointing: True\n",
        "# # Xformers must be installed\n",
        "# enable_xformers_memory_efficient_attention: False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iX1eFvYTLldG",
        "outputId": "c827cf29-a327-410d-cc66-0741fbc7047c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/luthando/miniconda3/envs/pytorch-gpu/lib/python3.12/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  torch.utils._pytree._register_pytree_node(\n",
            "/home/luthando/miniconda3/envs/pytorch-gpu/lib/python3.12/site-packages/diffusers/models/transformer_temporal.py:24: FutureWarning: `TransformerTemporalModelOutput` is deprecated and will be removed in version 0.29. Importing `TransformerTemporalModelOutput` from `diffusers.models.transformer_temporal` is deprecated and this will be removed in a future version. Please use `from diffusers.models.transformers.tranformer_temporal import TransformerTemporalModelOutput`, instead.\n",
            "  deprecate(\"TransformerTemporalModelOutput\", \"0.29\", deprecation_message)\n",
            "/home/luthando/miniconda3/envs/pytorch-gpu/lib/python3.12/site-packages/diffusers/models/transformer_temporal.py:29: FutureWarning: `TransformerTemporalModel` is deprecated and will be removed in version 0.29. Importing `TransformerTemporalModel` from `diffusers.models.transformer_temporal` is deprecated and this will be removed in a future version. Please use `from diffusers.models.transformers.tranformer_temporal import TransformerTemporalModel`, instead.\n",
            "  deprecate(\"TransformerTemporalModel\", \"0.29\", deprecation_message)\n",
            "/home/luthando/miniconda3/envs/pytorch-gpu/lib/python3.12/site-packages/diffusers/models/transformer_temporal.py:34: FutureWarning: `TransformerTemporalModelOutput` is deprecated and will be removed in version 0.29. Importing `TransformerSpatioTemporalModel` from `diffusers.models.transformer_temporal` is deprecated and this will be removed in a future version. Please use `from diffusers.models.transformers.tranformer_temporal import TransformerSpatioTemporalModel`, instead.\n",
            "  deprecate(\"TransformerTemporalModelOutput\", \"0.29\", deprecation_message)\n",
            "/home/luthando/miniconda3/envs/pytorch-gpu/lib/python3.12/site-packages/diffusers/models/transformer_2d.py:20: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 0.29. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.transformers.transformer_2d import Transformer2DModelOutput`, instead.\n",
            "  deprecate(\"Transformer2DModelOutput\", \"0.29\", deprecation_message)\n",
            "/home/luthando/miniconda3/envs/pytorch-gpu/lib/python3.12/site-packages/diffusers/models/transformer_2d.py:25: FutureWarning: `Transformer2DModel` is deprecated and will be removed in version 0.29. Importing `Transformer2DModel` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.transformers.transformer_2d import Transformer2DModel`, instead.\n",
            "  deprecate(\"Transformer2DModel\", \"0.29\", deprecation_message)\n",
            "/home/luthando/miniconda3/envs/pytorch-gpu/lib/python3.12/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  torch.utils._pytree._register_pytree_node(\n",
            "Initializing the conversion map\n",
            "/home/luthando/miniconda3/envs/pytorch-gpu/lib/python3.12/site-packages/accelerate/accelerator.py:394: UserWarning: `log_with=tensorboard` was passed but no supported trackers are currently installed.\n",
            "  warnings.warn(f\"`log_with={log_with}` was passed but no supported trackers are currently installed.\")\n",
            "02/22/2024 21:18:00 - INFO - __main__ - Distributed environment: DistributedType.NO\n",
            "Num processes: 1\n",
            "Process index: 0\n",
            "Local process index: 0\n",
            "Device: cuda\n",
            "\n",
            "Mixed precision type: fp16\n",
            "\n",
            "/home/luthando/miniconda3/envs/pytorch-gpu/lib/python3.12/site-packages/diffusers/configuration_utils.py:244: FutureWarning: It is deprecated to pass a pretrained model name or path to `from_config`.If you were trying to load a scheduler, please use <class 'diffusers.schedulers.scheduling_ddpm.DDPMScheduler'>.from_pretrained(...) instead. Otherwise, please make sure to pass a configuration dictionary instead. This functionality will be removed in v1.0.0.\n",
            "  deprecate(\"config-passed-as-path\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
            "{'variance_type', 'rescale_betas_zero_snr', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/luthando/Desktop/AutoVisual/train.py\", line 1025, in <module>\n",
            "    main(**config)\n",
            "  File \"/home/luthando/Desktop/AutoVisual/train.py\", line 557, in main\n",
            "    noise_scheduler, tokenizer, text_encoder, vae, unet = load_primary_models(pretrained_model_path)\n",
            "                                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/luthando/Desktop/AutoVisual/train.py\", line 127, in load_primary_models\n",
            "    text_encoder = CLIPTextModel.from_pretrained(pretrained_model_path, subfolder=\"text_encoder\")\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/luthando/miniconda3/envs/pytorch-gpu/lib/python3.12/site-packages/transformers/modeling_utils.py\", line 3503, in from_pretrained\n",
            "    with safe_open(resolved_archive_file, framework=\"pt\") as f:\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge\n"
          ]
        }
      ],
      "source": [
        "!python train.py --config ./configs/my_config_hq.yaml --experiment_num=0 --use_wandb=False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DotzCfQbShbf"
      },
      "source": [
        "# Test Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6iy3bIOhfgE",
        "outputId": "4296fae8-b85a-49e9-8198-27c29cf81f79"
      },
      "outputs": [],
      "source": [
        "# !python inference.py \\\n",
        "#     --model '/content/drive/MyDrive/Colab Notebooks/Supervised Learning/T2V/Text-To-Video-Finetuning/outputs/train_2024-01-13T20-29-09/checkpoint-50\" --prompt \"Simplify (x+1)(x-1) explained' \\\n",
        "#     --prompt 'Please simplify this expression: (x+1)(x-1)' \\\n",
        "#     --neg_prompt \"watermark+++, text, shutterstock text, shutterstock++, blurry, ugly, username, url, low resolution, low quality\" \\\n",
        "#     --num-frames 16 \\\n",
        "#     --window-size 12 \\\n",
        "#     --width 128 \\\n",
        "#     --height 128 \\\n",
        "#     --sdp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "q1DttecwShbf"
      },
      "outputs": [],
      "source": [
        "# import random\n",
        "# import numpy as np\n",
        "# import torch\n",
        "# from diffusers import DPMSolverMultistepScheduler, DDPMScheduler, TextToVideoSDPipeline\n",
        "# from diffusers.utils import export_to_video\n",
        "# import imageio\n",
        "# from compel import Compel\n",
        "\n",
        "# def to_video(fn: str, frames: list[np.ndarray], fps: int) -> str:\n",
        "#     # out_file = tempfile.NamedTemporaryFile(suffix='.mp4', delete=False)\n",
        "#     writer = imageio.get_writer(fn, format='FFMPEG', fps=fps)\n",
        "#     for frame in frames:\n",
        "#         writer.append_data(frame)\n",
        "#     writer.close()\n",
        "#     return fn\n",
        "\n",
        "# my_trained_model_path = f\"{WORKING_DIR}/Text-To-Video-Finetuning/outputs/train_2023-12-06T21-23-48/checkpoint-12500\"\n",
        "# # pipe = TextToVideoSDPipeline.from_pretrained(my_trained_model_path,torch_dtype=torch.float16, variant=\"fp16\")\n",
        "# # compel = Compel(tokenizer=pipe.tokenizer, text_encoder=pipe.text_encoder)\n",
        "# # pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
        "# # pipe.enable_model_cpu_offload()\n",
        "\n",
        "# # seed = random.randint(0, 1000000)\n",
        "# # generator = torch.Generator().manual_seed(seed)\n",
        "\n",
        "# # prompt = \"(x+1)(x-1) expression simplified\"\n",
        "# # neg_prompt = \"watermark+++, text, shutterstock text, shutterstock++, blurry, ugly, username, url, low resolution, low quality\"\n",
        "# # prompt_embeds = compel.build_conditioning_tensor(prompt)\n",
        "# # neg_prompt_embeds = compel.build_conditioning_tensor(neg_prompt)\n",
        "# # video_frames = pipe(prompt_embeds=prompt_embeds, negative_prompt_embeds=neg_prompt_embeds, num_frames=100, width=384, height=256, num_inference_steps=50, guidance_scale=9, generator=generator).frames\n",
        "\n",
        "# # prompt_string = prompt.replace(\" \", \"-\")\n",
        "# # out_file = f\"{WORKING_DIR}/\"+prompt_string+\"-seed\"+str(seed)+\".mp4\"\n",
        "# # video_path = to_video(out_file, video_frames, 24)\n",
        "# # video_path = export_to_video(video_frames, out_file)\n",
        "# # print(video_path+ \" is ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Mb0SZgJUShbg",
        "outputId": "cffc7720-ef56-4125-e94a-00826570bce4"
      },
      "outputs": [],
      "source": [
        "# !pip freeze | grep torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ipW13ByAShbg",
        "outputId": "3daf3dc6-568e-4ad7-f988-5b62fd8ba982"
      },
      "outputs": [],
      "source": [
        "# !pip freeze | grep diffusers"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuClass": "premium",
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
